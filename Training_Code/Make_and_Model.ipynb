{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QF0ds3I0w0qL",
        "outputId": "9b45b957-9b64-4a00-ccef-095f56726851"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# ðŸš€ STEP 0: SETUP\n",
        "# ===============================\n",
        "\n",
        "# Install Kaggle CLI if not installed\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Upload kaggle.json to authenticate Kaggle API\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Move kaggle.json to correct directory\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ STEP 1: DOWNLOAD & UNZIP DATASET\n",
        "# ===============================\n",
        "\n",
        "# Download Stanford Cars dataset (by classes folder)\n",
        "!kaggle datasets download -d jutrera/stanford-car-dataset-by-classes-folder\n",
        "\n",
        "# Unzip\n",
        "!unzip -q stanford-car-dataset-by-classes-folder.zip\n",
        "\n",
        "# Check contents\n",
        "!ls\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ STEP 2: IMPORT LIBRARIES\n",
        "# ===============================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ STEP 3: MOUNT GOOGLE DRIVE\n",
        "# ===============================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ STEP 4: DATA GENERATORS\n",
        "# ===============================\n",
        "\n",
        "train_dir = '/content/car_data/car_data/train'\n",
        "test_dir = '/content/car_data/car_data/test'\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ STEP 5: BUILD MODEL (RESNET50)\n",
        "# ===============================\n",
        "\n",
        "base_model = ResNet50(input_shape=IMG_SIZE + (3,), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False  # Freeze base model initially\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ STEP 6: TRAIN MODEL (FROZEN BASE)\n",
        "# ===============================\n",
        "\n",
        "# Define callbacks to save model to Google Drive\n",
        "checkpoint_path_frozen = '/content/drive/MyDrive/stanford_car_resnet50_model_frozen.h5'\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path_frozen,\n",
        "                                                monitor='val_loss',\n",
        "                                                save_best_only=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ STEP 7: FINE-TUNE MODEL\n",
        "# ===============================\n",
        "\n",
        "# Unfreeze base model for fine-tuning\n",
        "base_model.trainable = True\n",
        "\n",
        "# Compile with lower learning rate for fine-tuning\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define fine-tuning checkpoint path on Drive\n",
        "checkpoint_path_finetuned = '/content/drive/MyDrive/stanford_car_resnet50_model_finetuned.h5'\n",
        "\n",
        "checkpoint_ft = tf.keras.callbacks.ModelCheckpoint(checkpoint_path_finetuned,\n",
        "                                                   monitor='val_loss',\n",
        "                                                   save_best_only=True)\n",
        "\n",
        "history_ft = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stop, checkpoint_ft]\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ STEP 8: PLOT TRAINING CURVES\n",
        "# ===============================\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train Acc (Frozen)')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc (Frozen)')\n",
        "plt.plot(history_ft.history['accuracy'], label='Train Acc (Finetuned)')\n",
        "plt.plot(history_ft.history['val_accuracy'], label='Val Acc (Finetuned)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ STEP 9: EVALUATE FINAL MODEL\n",
        "# ===============================\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test Accuracy after fine-tuning: {test_acc*100:.2f}%\")\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ STEP 10: SAVE FINAL MODEL TO DRIVE (OPTIONAL)\n",
        "# ===============================\n",
        "\n",
        "final_model_path = '/content/drive/MyDrive/stanford_car_resnet50_model_final.h5'\n",
        "model.save(final_model_path)\n",
        "\n",
        "print(f\"âœ… Final model saved to: {final_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "SFpKd1u8sNOn",
        "outputId": "3ef0fe8a-4e8c-4ecc-e74f-2a53e5568c11"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# ðŸš€ FIX: ENSURE DATASET FOLDERS EXIST\n",
        "# ===============================\n",
        "\n",
        "import os\n",
        "\n",
        "# Check if folder exists, if not, download and unzip\n",
        "if not os.path.exists('/content/car_data/car_data/train'):\n",
        "    print(\"ðŸ”„ Dataset not found. Downloading and unzipping...\")\n",
        "\n",
        "    # Install Kaggle if needed\n",
        "    !pip install -q kaggle\n",
        "\n",
        "    # Authenticate Kaggle\n",
        "    from google.colab import files\n",
        "    files.upload()  # Upload kaggle.json again if needed\n",
        "\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    # Download dataset\n",
        "    !kaggle datasets download -d jutrera/stanford-car-dataset-by-classes-folder\n",
        "\n",
        "    # Unzip dataset\n",
        "    !unzip -q stanford-car-dataset-by-classes-folder.zip\n",
        "\n",
        "    print(\"âœ… Dataset ready.\")\n",
        "else:\n",
        "    print(\"âœ… Dataset folder already exists.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GIl1-0XtqDuj",
        "outputId": "e96d30c8-04eb-4b77-c751-5f1228667b99"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# ðŸš€ CONTINUATION TRAINING CODE\n",
        "# ===============================\n",
        "\n",
        "# ðŸ”· Install Kaggle (if not already installed)\n",
        "!pip install -q kaggle\n",
        "\n",
        "# ðŸ”· Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ðŸ”· Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ RELOAD DATASET\n",
        "# ===============================\n",
        "\n",
        "# Download dataset if not already in session\n",
        "!kaggle datasets download -d jutrera/stanford-car-dataset-by-classes-folder\n",
        "!unzip -q stanford-car-dataset-by-classes-folder.zip\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ DATA GENERATORS\n",
        "# ===============================\n",
        "\n",
        "train_dir = '/content/car_data/car_data/train'\n",
        "test_dir = '/content/car_data/car_data/test'\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ LOAD SAVED FINETUNED MODEL\n",
        "# ===============================\n",
        "\n",
        "model_path = '/content/drive/MyDrive/stanford_car_resnet50_model_finetuned.h5'  # adjust if your path differs\n",
        "model = load_model(model_path)\n",
        "model.summary()\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ PREPARE FOR CONTINUED TRAINING\n",
        "# ===============================\n",
        "\n",
        "# Ensure base model is trainable\n",
        "base_model = model.layers[0]\n",
        "base_model.trainable = True\n",
        "\n",
        "# Compile with low learning rate for continued fine-tuning\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ CONTINUE TRAINING\n",
        "# ===============================\n",
        "\n",
        "checkpoint_path_continue = '/content/drive/MyDrive/stanford_car_resnet50_model_finetuned_continued.h5'\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path_continue,\n",
        "                                                monitor='val_loss',\n",
        "                                                save_best_only=True)\n",
        "\n",
        "history_continue = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,  # adjust as needed\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ PLOT CONTINUED TRAINING CURVES\n",
        "# ===============================\n",
        "\n",
        "plt.plot(history_continue.history['accuracy'], label='Train Acc (Continued)')\n",
        "plt.plot(history_continue.history['val_accuracy'], label='Val Acc (Continued)')\n",
        "plt.title('Continued Fine-Tuning Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ FINAL EVALUATION\n",
        "# ===============================\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"âœ… Test Accuracy after continued fine-tuning: {test_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6hM9MXFFXieb",
        "outputId": "6c1577b5-62ee-4252-a225-7cd72d1b6435"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# ðŸš€ CONTINUE TRAINING FOR 20 MORE EPOCHS (FULL SAFE VERSION)\n",
        "# ===============================\n",
        "\n",
        "# ðŸ”· Install Kaggle if needed\n",
        "!pip install -q kaggle\n",
        "\n",
        "# ðŸ”· Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ MOUNT GOOGLE DRIVE\n",
        "# ===============================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ ENSURE DATASET IS AVAILABLE\n",
        "# ===============================\n",
        "\n",
        "# Check if dataset exists, if not, download and unzip\n",
        "if not os.path.exists('/content/car_data/car_data/train'):\n",
        "    print(\"ðŸ”„ Dataset not found. Downloading and unzipping...\")\n",
        "\n",
        "    # Upload kaggle.json if needed\n",
        "    from google.colab import files\n",
        "    files.upload()\n",
        "\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    # Download dataset\n",
        "    !kaggle datasets download -d jutrera/stanford-car-dataset-by-classes-folder\n",
        "    !unzip -q stanford-car-dataset-by-classes-folder.zip\n",
        "\n",
        "    print(\"âœ… Dataset ready.\")\n",
        "else:\n",
        "    print(\"âœ… Dataset already exists.\")\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ DATA GENERATORS\n",
        "# ===============================\n",
        "\n",
        "train_dir = '/content/car_data/car_data/train'\n",
        "test_dir = '/content/car_data/car_data/test'\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ LOAD YOUR LAST SAVED MODEL\n",
        "# ===============================\n",
        "\n",
        "model_path = '/content/drive/MyDrive/stanford_car_resnet50_model_finetuned_continued.h5'\n",
        "model = load_model(model_path)\n",
        "model.summary()\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ ENSURE BASE MODEL IS TRAINABLE\n",
        "# ===============================\n",
        "\n",
        "base_model = model.layers[0]\n",
        "base_model.trainable = True\n",
        "\n",
        "# Re-compile with low learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ DEFINE CHECKPOINT & EARLY STOPPING\n",
        "# ===============================\n",
        "\n",
        "checkpoint_path_continue_20 = '/content/drive/MyDrive/stanford_car_resnet50_model_finetuned_continued_20more.h5'\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path_continue_20,\n",
        "                                                monitor='val_loss',\n",
        "                                                save_best_only=True)\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ TRAIN FOR 20 MORE EPOCHS\n",
        "# ===============================\n",
        "\n",
        "history_continue = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ PLOT TRAINING CURVES\n",
        "# ===============================\n",
        "\n",
        "plt.plot(history_continue.history['accuracy'], label='Train Acc (Continued 20)')\n",
        "plt.plot(history_continue.history['val_accuracy'], label='Val Acc (Continued 20)')\n",
        "plt.title('Continued Fine-Tuning Accuracy (20 more epochs)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ FINAL EVALUATION\n",
        "# ===============================\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"âœ… Test Accuracy after additional 20 epochs: {test_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "JAR9kGoqRH1E",
        "outputId": "c110fbfb-0ac8-46d0-9458-975479331d87"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# ðŸ”§ Stanford Cars MMC Inference +\n",
        "# ðŸ”¢ Manual Accuracy Evaluation\n",
        "# ================================\n",
        "\n",
        "# 1. Install requirements\n",
        "!pip install -q tensorflow\n",
        "\n",
        "# 2. Import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from google.colab import drive, files\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# 3. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 4. Load your trained model\n",
        "model_path = '/content/drive/MyDrive/stanford_car_resnet50_model_finetuned_continued_20more.h5'\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "print(f\"âœ… Model loaded from: {model_path}\")\n",
        "\n",
        "# 5. Load class labels (based on your training data)\n",
        "# Update this if your dataset structure is different\n",
        "train_dir = '/content/car_data/car_data/train'  # Change if needed\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "generator = datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=1, class_mode='categorical')\n",
        "class_indices = generator.class_indices\n",
        "class_labels = list(class_indices.keys())\n",
        "print(\"âœ… Class labels loaded.\")\n",
        "\n",
        "# 6. Upload multiple images\n",
        "uploaded = files.upload()\n",
        "image_filenames = list(uploaded.keys())\n",
        "\n",
        "# 7. Predict each image and optionally collect accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# ðŸ” Optional: Map your uploaded image filenames to their correct class (ground truth)\n",
        "# Example: {'car1.jpg': 'Aston Martin DB9 Coupe 2012', ...}\n",
        "ground_truth = {\n",
        "    'img1.jpg': 'Aston Martin DB9 Coupe 2012',\n",
        "    'img2.jpg': 'Honda Accord Sedan 2012',\n",
        "    'img3.jpg': 'Tesla Model S 2012'\n",
        "    # ðŸ‘† Replace with your actual uploaded image filenames and true labels\n",
        "}\n",
        "\n",
        "for filename in image_filenames:\n",
        "    # Load and preprocess image\n",
        "    img = image.load_img(filename, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    # Predict\n",
        "    preds = model.predict(img_array)\n",
        "    pred_class_index = np.argmax(preds)\n",
        "    confidence = np.max(preds) * 100\n",
        "    predicted_label = class_labels[pred_class_index]\n",
        "\n",
        "    # Print result\n",
        "    print(f\"\\nðŸ–¼ï¸ Image: {filename}\")\n",
        "    print(f\"ðŸ” Predicted: {predicted_label} ({confidence:.2f}%)\")\n",
        "\n",
        "    # Display image\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Predicted: {predicted_label}\\nConfidence: {confidence:.2f}%\")\n",
        "    plt.show()\n",
        "\n",
        "    # Check against ground truth (if provided)\n",
        "    if filename in ground_truth:\n",
        "        actual_label = ground_truth[filename]\n",
        "        total += 1\n",
        "        if predicted_label == actual_label:\n",
        "            correct += 1\n",
        "        else:\n",
        "            print(f\"âŒ Wrong! Actual: {actual_label}\")\n",
        "\n",
        "# 8. Print final accuracy\n",
        "if total > 0:\n",
        "    acc = (correct / total) * 100\n",
        "    print(f\"\\nâœ… Accuracy on uploaded images: {acc:.2f}% ({correct}/{total})\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ No ground truth provided. Accuracy not calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "26MerBK7rzan",
        "outputId": "d6a556e7-ed92-4f02-bf38-4aa57cbaaa1d"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# ðŸš€ IMPORT LIBRARIES\n",
        "# ===============================\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ LOAD MODEL\n",
        "# ===============================\n",
        "model_path = '/content/drive/MyDrive/stanford_car_resnet50_model_finetuned_continued_20more.h5'\n",
        "model = load_model(model_path)\n",
        "model.summary()\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ LOAD LABELS (CLASS INDICES)\n",
        "# ===============================\n",
        "# Assuming your test set structure is still present\n",
        "test_dir = '/content/car_data/car_data/test'\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "class_indices = test_generator.class_indices\n",
        "idx_to_class = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ EVALUATE ACCURACY\n",
        "# ===============================\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"âœ… Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ UPLOAD IMAGES FOR PREDICTION\n",
        "# ===============================\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fname in uploaded.keys():\n",
        "    # Load and preprocess\n",
        "    img_path = fname\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = preprocess_input(img_array)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Predict\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = idx_to_class[np.argmax(predictions)]\n",
        "\n",
        "    # Plot\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"ðŸš— Predicted: {predicted_class}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "1WIg9UnKspeX",
        "outputId": "313bbf00-cae5-4818-c8f6-3f0d2e186238"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# ðŸ”¹ INSTALL & IMPORTS\n",
        "# ===============================\n",
        "!pip install -q kaggle\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ MOUNT GOOGLE DRIVE\n",
        "# ===============================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ DOWNLOAD & EXTRACT DATASET\n",
        "# ===============================\n",
        "if not os.path.exists(\"/content/car_data/car_data/train\"):\n",
        "    print(\"ðŸ”„ Downloading and extracting Stanford Cars dataset...\")\n",
        "\n",
        "    uploaded = files.upload()  # Upload your kaggle.json\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    !kaggle datasets download -d jutrera/stanford-car-dataset-by-classes-folder\n",
        "    !unzip -q stanford-car-dataset-by-classes-folder.zip -d /content/car_data\n",
        "\n",
        "    print(\"âœ… Dataset downloaded and extracted to /content/car_data\")\n",
        "else:\n",
        "    print(\"âœ… Dataset already exists at /content/car_data\")\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ LOAD CLASS LABELS\n",
        "# ===============================\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "temp_datagen = ImageDataGenerator()\n",
        "temp_generator = temp_datagen.flow_from_directory(\n",
        "    '/content/car_data/car_data/train',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=1,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "class_indices = temp_generator.class_indices\n",
        "class_labels = list(class_indices.keys())\n",
        "\n",
        "print(f\"âœ… Loaded {len(class_labels)} class labels.\")\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ LOAD MODEL FROM DRIVE\n",
        "# ===============================\n",
        "model_path = '/content/drive/MyDrive/stanford_car_resnet50_model_finetuned_continued_20more.h5'\n",
        "\n",
        "model = load_model(model_path)\n",
        "print(\"âœ… Model loaded successfully.\")\n",
        "model.summary()\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ UPLOAD IMAGE(S) FOR PREDICTION\n",
        "# ===============================\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fname in uploaded.keys():\n",
        "    # Load and preprocess image\n",
        "    img_path = os.path.join(\"/content\", fname)\n",
        "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    # Predict\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_index = np.argmax(predictions)\n",
        "    predicted_class = class_labels[predicted_index]\n",
        "    confidence = np.max(predictions)\n",
        "\n",
        "    # Display\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Predicted: {predicted_class} ({confidence * 100:.2f}%)\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "2jXfPfOF3tqa",
        "outputId": "5dbdda06-7f56-45ff-d422-dd76183e3103"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fname in uploaded.keys():\n",
        "    img_path = os.path.join(\"/content\", fname)\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = preprocess_input(np.expand_dims(img_array, axis=0))\n",
        "\n",
        "    preds = model.predict(img_array)\n",
        "    top_index = np.argmax(preds)\n",
        "    predicted_class = class_names[top_index]\n",
        "    confidence = preds[0][top_index]\n",
        "\n",
        "    # Show result\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Prediction: {predicted_class} ({confidence*100:.2f}%)\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "t4tQCq-U2aOU",
        "outputId": "14c34552-9be6-4df6-b62c-7105d6c6b51d"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# ðŸ”¹ INSTALL DEPENDENCIES\n",
        "# ===============================\n",
        "!pip install -q tensorflow matplotlib\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive, files\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ MOUNT GOOGLE DRIVE\n",
        "# ===============================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ DOWNLOAD & EXTRACT DATASET\n",
        "# ===============================\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "dataset_dir = \"/content/stanford_cars\"\n",
        "if not os.path.exists(dataset_dir):\n",
        "    os.makedirs(dataset_dir)\n",
        "\n",
        "if not os.path.exists(os.path.join(dataset_dir, \"cars_train\")):\n",
        "    print(\"ðŸ”½ Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(\n",
        "        \"http://ai.stanford.edu/~jkrause/car196/cars_train.tgz\",\n",
        "        os.path.join(dataset_dir, \"cars_train.tgz\")\n",
        "    )\n",
        "    urllib.request.urlretrieve(\n",
        "        \"http://ai.stanford.edu/~jkrause/car196/cars_test.tgz\",\n",
        "        os.path.join(dataset_dir, \"cars_test.tgz\")\n",
        "    )\n",
        "    urllib.request.urlretrieve(\n",
        "        \"http://ai.stanford.edu/~jkrause/car196/cars_train_annos.mat\",\n",
        "        os.path.join(dataset_dir, \"cars_train_annos.mat\")\n",
        "    )\n",
        "    urllib.request.urlretrieve(\n",
        "        \"http://ai.stanford.edu/~jkrause/car196/cars_test_annos_withlabels.mat\",\n",
        "        os.path.join(dataset_dir, \"cars_test_annos_withlabels.mat\")\n",
        "    )\n",
        "    urllib.request.urlretrieve(\n",
        "        \"http://ai.stanford.edu/~jkrause/car196/cars_meta.mat\",\n",
        "        os.path.join(dataset_dir, \"cars_meta.mat\")\n",
        "    )\n",
        "\n",
        "    print(\"ðŸ“¦ Extracting...\")\n",
        "    with tarfile.open(os.path.join(dataset_dir, \"cars_train.tgz\")) as tar:\n",
        "        tar.extractall(path=dataset_dir)\n",
        "    with tarfile.open(os.path.join(dataset_dir, \"cars_test.tgz\")) as tar:\n",
        "        tar.extractall(path=dataset_dir)\n",
        "\n",
        "    print(\"âœ… Downloaded and extracted Stanford Cars dataset\")\n",
        "else:\n",
        "    print(\"âœ… Dataset already downloaded.\")\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ LOAD CLASS LABELS\n",
        "# ===============================\n",
        "meta = scipy.io.loadmat(os.path.join(dataset_dir, \"cars_meta.mat\"))\n",
        "class_names = [x[0] for x in meta[\"class_names\"][0]]\n",
        "print(f\"âœ… Loaded {len(class_names)} classes.\")\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ LOAD YOUR TRAINED MODEL\n",
        "# ===============================\n",
        "model_path = '/content/drive/MyDrive/stanford_car_resnet50_model_finetuned_continued_20more.h5'\n",
        "model = load_model(model_path)\n",
        "print(\"âœ… Model loaded.\")\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ UPLOAD & PREDICT\n",
        "# ===============================\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fname in uploaded.keys():\n",
        "    img_path = os.path.join(\"/content\", fname)\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = preprocess_input(np.expand_dims(img_array, axis=0))\n",
        "\n",
        "    preds = model.predict(img_array)\n",
        "    top_index = np.argmax(preds)\n",
        "    predicted_class = class_names[top_index]\n",
        "    confidence = preds[0][top_index]\n",
        "\n",
        "    # Show result\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Prediction: {predicted_class} ({confidence*100:.2f}%)\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tfc_Rr5s46yu",
        "outputId": "6e1496ff-0617-4718-c1dd-f3f5e37f6948"
      },
      "outputs": [],
      "source": [
        "# === Step 1: Setup Kaggle & Download Dataset ===\n",
        "!pip install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()  # Upload your kaggle.json here\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download Stanford Cars dataset\n",
        "!kaggle datasets download -d jutrera/stanford-car-dataset-by-classes-folder\n",
        "\n",
        "# Unzip dataset\n",
        "!unzip -q stanford-car-dataset-by-classes-folder.zip\n",
        "\n",
        "# === Step 2: Extract class names from train folder ===\n",
        "import os\n",
        "\n",
        "train_dir = '/content/car_data/car_data/train'\n",
        "class_names = sorted(os.listdir(train_dir))\n",
        "print(f\"âœ… Found {len(class_names)} classes.\")\n",
        "print(class_names[:10])  # show first 10 classes as example\n",
        "\n",
        "# === Step 3: Load libraries & your model ===\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path = '/content/drive/MyDrive/stanford_car_resnet50_model_finetuned_continued_20more.h5'\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "print(\"âœ… Model loaded.\")\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "# === Step 4: Upload image(s) for prediction ===\n",
        "print(\"Upload image(s) to predict the make and model:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for img_name in uploaded.keys():\n",
        "    print(f\"\\nProcessing {img_name}...\")\n",
        "\n",
        "    # Load and preprocess image\n",
        "    img = image.load_img(img_name, target_size=IMG_SIZE)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array_exp = np.expand_dims(img_array, axis=0)\n",
        "    img_preprocessed = preprocess_input(img_array_exp)\n",
        "\n",
        "    # Predict\n",
        "    preds = model.predict(img_preprocessed)\n",
        "    pred_index = np.argmax(preds)\n",
        "    pred_class = class_names[pred_index]\n",
        "    confidence = preds[0][pred_index]\n",
        "\n",
        "    # Print prediction and confidence\n",
        "    print(f\"Predicted Make & Model: {pred_class}\")\n",
        "    print(f\"Confidence: {confidence * 100:.2f}%\")\n",
        "\n",
        "    # Display image with prediction\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Prediction: {pred_class}\\nConfidence: {confidence*100:.2f}%\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EN_Qx9tJ_AuO",
        "outputId": "4a0f2077-bd1b-47b8-c94b-6f9c03f41b45"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "\n",
        "# Upload multiple images\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "for image_path in uploaded_files.keys():\n",
        "    print(f\"\\nProcessing: {image_path}\")\n",
        "\n",
        "    # Load image and preprocess\n",
        "    img = Image.open(image_path).convert('RGB').resize((224, 224))\n",
        "    img_array = np.array(img)\n",
        "    img_preprocessed = preprocess_input(np.expand_dims(img_array, axis=0))  # Add batch dim\n",
        "\n",
        "    # Predict\n",
        "    preds = model.predict(img_preprocessed)\n",
        "\n",
        "    # Get top 3 predictions\n",
        "    top_k = 3\n",
        "    top_indices = preds[0].argsort()[-top_k:][::-1]\n",
        "\n",
        "    print(\"Top 3 predictions:\")\n",
        "    for i in top_indices:\n",
        "        print(f\"{class_names[i]}: {preds[0][i]*100:.2f}%\")\n",
        "\n",
        "    # Optional: display image with highest prediction label\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Top prediction: {class_names[top_indices[0]]}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YNRGCx2JExv",
        "outputId": "a647cf95-3313-4954-d8f2-cc83ec5baef6"
      },
      "outputs": [],
      "source": [
        "# --- convert_h5_to_pt_drive.py ---\n",
        "import torch\n",
        "from torchvision import models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Step 0: Mount Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Step 1: Load Keras model from Drive ---\n",
        "keras_model_path = \"/content/drive/MyDrive/stanford_car_resnet50_model_finetuned_continued_20more.h5\"\n",
        "keras_model = load_model(keras_model_path)\n",
        "print(\"âœ… Keras model loaded from Drive.\")\n",
        "\n",
        "# --- Step 2: Rebuild same ResNet50 architecture in PyTorch ---\n",
        "num_classes = keras_model.output_shape[1]  # should be 196 for Cars196\n",
        "pt_model = models.resnet50(pretrained=False)\n",
        "pt_model.fc = torch.nn.Linear(pt_model.fc.in_features, num_classes)\n",
        "\n",
        "# --- Step 3: Copy weights from Keras to PyTorch ---\n",
        "# Function to copy weights layer by layer\n",
        "keras_layers = [layer for layer in keras_model.layers if len(layer.get_weights()) > 0]\n",
        "\n",
        "for layer in keras_layers:\n",
        "    weights = layer.get_weights()\n",
        "    if 'conv' in layer.name and len(weights) > 0:\n",
        "        w = np.transpose(weights[0], (3, 2, 0, 1))\n",
        "        pt_layer = dict(pt_model.named_modules()).get(layer.name)\n",
        "        if pt_layer is not None:\n",
        "            pt_layer.weight.data = torch.from_numpy(w).float()\n",
        "        if len(weights) > 1:\n",
        "            pt_layer.bias.data = torch.from_numpy(weights[1]).float()\n",
        "    elif 'bn' in layer.name and len(weights) > 0:\n",
        "        pt_layer = dict(pt_model.named_modules()).get(layer.name)\n",
        "        if pt_layer is not None:\n",
        "            pt_layer.weight.data = torch.from_numpy(weights[0]).float()\n",
        "            pt_layer.bias.data = torch.from_numpy(weights[1]).float()\n",
        "            pt_layer.running_mean = torch.from_numpy(weights[2]).float()\n",
        "            pt_layer.running_var = torch.from_numpy(weights[3]).float()\n",
        "    elif 'dense' in layer.name or 'fc' in layer.name:\n",
        "        w, b = weights\n",
        "        pt_model.fc.weight.data = torch.from_numpy(w.T).float()\n",
        "        pt_model.fc.bias.data = torch.from_numpy(b).float()\n",
        "\n",
        "print(\"âœ… Weights copied to PyTorch model.\")\n",
        "\n",
        "# --- Step 4: Save PyTorch model back to Drive ---\n",
        "pt_model_path = \"/content/drive/MyDrive/stanford_car_resnet50_model_finetuned.pt\"\n",
        "torch.save(pt_model, pt_model_path)\n",
        "print(f\"âœ… PyTorch model saved to Drive at {pt_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "taNC1kGJD4r1",
        "outputId": "567f232d-608f-49b6-a29f-0a23240cf8a0"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ðŸš€ Convert Keras .h5 to TFLite and SAVE TO DRIVE\n",
        "# ============================================\n",
        "from google.colab import drive\n",
        "import os, urllib.request, scipy.io\n",
        "import tensorflow as tf\n",
        "\n",
        "# ---- 1) Mount Google Drive ----\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ---- 2) Configure your paths (EDIT if your filenames/folders differ) ----\n",
        "MODEL_H5 = '/content/drive/MyDrive/stanford_car_resnet50_model_finetuned_continued_20more.h5'\n",
        "DRIVE_OUT_DIR = '/content/drive/MyDrive/exports_for_pi'   # output folder in Drive\n",
        "CARS_META_URL = 'http://ai.stanford.edu/~jkrause/car196/cars_meta.mat'\n",
        "\n",
        "# Create output directory in Drive\n",
        "os.makedirs(DRIVE_OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---- 3) Verify model exists on Drive ----\n",
        "if not os.path.exists(MODEL_H5):\n",
        "    raise FileNotFoundError(f\"âŒ Model .h5 not found at: {MODEL_H5}\\n\"\n",
        "                            f\"âž¡ï¸  Update MODEL_H5 to the correct path in your Drive.\")\n",
        "\n",
        "print(f\"âœ… Found model: {MODEL_H5}\")\n",
        "\n",
        "# ---- 4) Ensure we have cars_meta.mat (for labels) ----\n",
        "meta_local_path = '/content/cars_meta.mat'\n",
        "if not os.path.exists(meta_local_path):\n",
        "    print(\"ðŸ“¥ Downloading cars_meta.mat for class labels...\")\n",
        "    urllib.request.urlretrieve(CARS_META_URL, meta_local_path)\n",
        "else:\n",
        "    print(\"âœ… cars_meta.mat already present locally\")\n",
        "\n",
        "# ---- 5) Build labels.txt directly in Drive ----\n",
        "meta = scipy.io.loadmat(meta_local_path)\n",
        "class_names = [x[0] for x in meta[\"class_names\"][0]]\n",
        "\n",
        "labels_drive_path = os.path.join(DRIVE_OUT_DIR, 'labels.txt')\n",
        "with open(labels_drive_path, 'w') as f:\n",
        "    f.writelines(name + '\\n' for name in class_names)\n",
        "\n",
        "print(f\"âœ… Saved labels.txt to Drive: {labels_drive_path}\")\n",
        "\n",
        "# ---- 6) Load .h5 and convert to TFLite (float model; optimized) ----\n",
        "print(\"ðŸ“¦ Loading Keras model...\")\n",
        "model = tf.keras.models.load_model(MODEL_H5)\n",
        "\n",
        "print(\"ðŸ”„ Converting to TensorFlow Lite...\")\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# Keeps float precision; applies graph/transformation optimizations\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# ---- 7) Save TFLite directly to Drive ----\n",
        "tflite_drive_path = os.path.join(DRIVE_OUT_DIR, 'cars_model.tflite')\n",
        "with open(tflite_drive_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"âœ… Saved TFLite model to Drive: {tflite_drive_path}\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ DONE! Files in Drive:\")\n",
        "print(f\"   â€¢ {tflite_drive_path}\")\n",
        "print(f\"   â€¢ {labels_drive_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "lsohVgfkm_jM",
        "outputId": "adda98c3-59bd-4294-ce0f-15e4db18ec06"
      },
      "outputs": [],
      "source": [
        "# === Setup Kaggle & Download Stanford Car Dataset ===\n",
        "\n",
        "import os\n",
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Install Kaggle API\n",
        "!pip install -q kaggle\n",
        "\n",
        "# 2. Upload your kaggle.json (API key)\n",
        "uploaded = files.upload()  # Select kaggle.json\n",
        "\n",
        "# 3. Move it to correct place\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 4. Download the Stanford Cars dataset (by classes folder)\n",
        "DATASET = \"jutrera/stanford-car-dataset-by-classes-folder\"\n",
        "!kaggle datasets download -d {DATASET} -p /content/\n",
        "\n",
        "# 5. Unzip the downloaded file\n",
        "!unzip -q /content/stanford-car-dataset-by-classes-folder.zip -d /content/car_data\n",
        "\n",
        "# 6. Define path to training folder (adjust if folder structure is different)\n",
        "train_dir = \"/content/car_data/car_data/train\"\n",
        "\n",
        "# 7. Read class names\n",
        "class_names = sorted(os.listdir(train_dir))\n",
        "print(f\"âœ… Found {len(class_names)} classes.\")\n",
        "print(\"First 10 classes:\", class_names[:10])\n",
        "\n",
        "# 8. Save class names in JSON\n",
        "with open(\"car_classes.json\", \"w\") as f:\n",
        "    json.dump(class_names, f)\n",
        "\n",
        "print(\"âœ… car_classes.json saved\")\n",
        "\n",
        "# 9. Download the JSON file to your local machine\n",
        "files.download(\"car_classes.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "613VYbGFpch4",
        "outputId": "bf653dc6-f6ff-491c-f7dd-8909958808a0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "# After unzipping, look through car_data directory for the right train folder\n",
        "base_dir = \"/content/car_data\"\n",
        "\n",
        "# Walk through folders and find one that looks like \"train\" or \"cars_train\"\n",
        "train_dir = None\n",
        "for root, dirs, files_in_dir in os.walk(base_dir):\n",
        "    for d in dirs:\n",
        "        if \"train\" in d.lower():  # catches \"train\", \"cars_train\", etc.\n",
        "            train_dir = os.path.join(root, d)\n",
        "            break\n",
        "    if train_dir:\n",
        "        break\n",
        "\n",
        "if train_dir is None:\n",
        "    raise FileNotFoundError(\"âŒ Could not find a training folder. Please check dataset structure.\")\n",
        "\n",
        "print(f\"âœ… Training directory found: {train_dir}\")\n",
        "\n",
        "# Get class names\n",
        "class_names = sorted(os.listdir(train_dir))\n",
        "print(f\"âœ… Found {len(class_names)} classes.\")\n",
        "print(\"First 10 classes:\", class_names[:10])\n",
        "\n",
        "# Save class names into JSON\n",
        "with open(\"car_classes.json\", \"w\") as f:\n",
        "    json.dump(class_names, f)\n",
        "\n",
        "print(\"âœ… car_classes.json saved\")\n",
        "\n",
        "# Download JSON to your computer\n",
        "files.download(\"car_classes.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "OjLWZJd-p0Ul",
        "outputId": "b56a1323-9690-4e8d-e2bc-c9e9264e42b0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Base unzip directory\n",
        "base_dir = \"/content/car_data\"\n",
        "\n",
        "# ðŸ” Auto-detect training folder\n",
        "train_dir = None\n",
        "for root, dirs, files_in_dir in os.walk(base_dir):\n",
        "    for d in dirs:\n",
        "        if \"train\" in d.lower():  # catches \"train\", \"cars_train\", etc.\n",
        "            train_dir = os.path.join(root, d)\n",
        "            break\n",
        "    if train_dir:\n",
        "        break\n",
        "\n",
        "if train_dir is None:\n",
        "    raise FileNotFoundError(\"âŒ Could not find a training folder. Please check dataset structure.\")\n",
        "\n",
        "print(f\"âœ… Training directory found: {train_dir}\")\n",
        "\n",
        "# ðŸ“‚ Read class names\n",
        "class_names = sorted(os.listdir(train_dir))\n",
        "print(f\"âœ… Found {len(class_names)} classes.\")\n",
        "print(\"First 10 classes:\", class_names[:10])\n",
        "\n",
        "# ðŸ“ Save to TXT (one class per line)\n",
        "with open(\"car_classes.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(class_names))\n",
        "\n",
        "print(\"âœ… car_classes.txt saved\")\n",
        "\n",
        "# ðŸ’¾ Download to your computer\n",
        "files.download(\"car_classes.txt\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
